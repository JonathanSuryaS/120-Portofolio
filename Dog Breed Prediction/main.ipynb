{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 64, 64\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6391 images belonging to 93 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,     \n",
    "    shear_range=0.2,         \n",
    "    zoom_range=0.2,      \n",
    "    horizontal_flip=True   \n",
    ")\n",
    "\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    'Dog Breed Classification/train',\n",
    "    target_size=(img_width, img_height),  \n",
    "    batch_size=batch_size,  \n",
    "    class_mode='categorical'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 887 images belonging to 93 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,     \n",
    "    shear_range=0.2,         \n",
    "    zoom_range=0.2,      \n",
    "    horizontal_flip=True   \n",
    ")\n",
    "\n",
    "\n",
    "test_set = train_datagen.flow_from_directory(\n",
    "    'Dog Breed Classification/test',\n",
    "    target_size=(img_width, img_height),  \n",
    "    batch_size=batch_size,  \n",
    "    class_mode='categorical'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 762 images belonging to 93 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,     \n",
    "    shear_range=0.2,         \n",
    "    zoom_range=0.2,      \n",
    "    horizontal_flip=True   \n",
    ")\n",
    "\n",
    "\n",
    "val_set = train_datagen.flow_from_directory(\n",
    "    'Dog Breed Classification/val',\n",
    "    target_size=(img_width, img_height),  \n",
    "    batch_size=batch_size,  \n",
    "    class_mode='categorical'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0694 - accuracy: 0.0146 - val_loss: 0.0593 - val_accuracy: 0.0184\n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0585 - accuracy: 0.0274 - val_loss: 0.0584 - val_accuracy: 0.0276\n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0567 - accuracy: 0.0446 - val_loss: 0.0564 - val_accuracy: 0.0420\n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0553 - accuracy: 0.0610 - val_loss: 0.0556 - val_accuracy: 0.0459\n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0541 - accuracy: 0.0735 - val_loss: 0.0549 - val_accuracy: 0.0617\n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0531 - accuracy: 0.0909 - val_loss: 0.0541 - val_accuracy: 0.0774\n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0521 - accuracy: 0.1047 - val_loss: 0.0539 - val_accuracy: 0.0643\n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0509 - accuracy: 0.1233 - val_loss: 0.0534 - val_accuracy: 0.0879\n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0501 - accuracy: 0.1316 - val_loss: 0.0532 - val_accuracy: 0.0945\n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0491 - accuracy: 0.1547 - val_loss: 0.0534 - val_accuracy: 0.0906\n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0481 - accuracy: 0.1713 - val_loss: 0.0532 - val_accuracy: 0.0971\n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0473 - accuracy: 0.1821 - val_loss: 0.0542 - val_accuracy: 0.0984\n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0462 - accuracy: 0.2025 - val_loss: 0.0540 - val_accuracy: 0.1010\n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0452 - accuracy: 0.2264 - val_loss: 0.0546 - val_accuracy: 0.1037\n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0443 - accuracy: 0.2378 - val_loss: 0.0545 - val_accuracy: 0.1207\n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0435 - accuracy: 0.2525 - val_loss: 0.0546 - val_accuracy: 0.1168\n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0425 - accuracy: 0.2735 - val_loss: 0.0550 - val_accuracy: 0.1129\n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0416 - accuracy: 0.2873 - val_loss: 0.0555 - val_accuracy: 0.1024\n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0403 - accuracy: 0.3147 - val_loss: 0.0559 - val_accuracy: 0.0984\n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0397 - accuracy: 0.3283 - val_loss: 0.0570 - val_accuracy: 0.1024\n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0388 - accuracy: 0.3441 - val_loss: 0.0585 - val_accuracy: 0.0958\n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0379 - accuracy: 0.3549 - val_loss: 0.0576 - val_accuracy: 0.1115\n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0371 - accuracy: 0.3713 - val_loss: 0.0583 - val_accuracy: 0.1063\n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0363 - accuracy: 0.3882 - val_loss: 0.0589 - val_accuracy: 0.1076\n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0356 - accuracy: 0.4023 - val_loss: 0.0596 - val_accuracy: 0.0945\n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0347 - accuracy: 0.4204 - val_loss: 0.0595 - val_accuracy: 0.1115\n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0340 - accuracy: 0.4350 - val_loss: 0.0609 - val_accuracy: 0.1102\n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0332 - accuracy: 0.4436 - val_loss: 0.0611 - val_accuracy: 0.1207\n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0325 - accuracy: 0.4663 - val_loss: 0.0615 - val_accuracy: 0.1115\n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0320 - accuracy: 0.4722 - val_loss: 0.0640 - val_accuracy: 0.0971\n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0315 - accuracy: 0.4790 - val_loss: 0.0654 - val_accuracy: 0.1050\n",
      "Epoch 32/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0303 - accuracy: 0.4987 - val_loss: 0.0658 - val_accuracy: 0.1037\n",
      "Epoch 33/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0302 - accuracy: 0.5126 - val_loss: 0.0653 - val_accuracy: 0.1089\n",
      "Epoch 34/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0295 - accuracy: 0.5204 - val_loss: 0.0661 - val_accuracy: 0.1234\n",
      "Epoch 35/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0290 - accuracy: 0.5337 - val_loss: 0.0676 - val_accuracy: 0.1050\n",
      "Epoch 36/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0287 - accuracy: 0.5376 - val_loss: 0.0694 - val_accuracy: 0.1010\n",
      "Epoch 37/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0284 - accuracy: 0.5494 - val_loss: 0.0703 - val_accuracy: 0.1168\n",
      "Epoch 38/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0276 - accuracy: 0.5606 - val_loss: 0.0720 - val_accuracy: 0.0997\n",
      "Epoch 39/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0271 - accuracy: 0.5650 - val_loss: 0.0723 - val_accuracy: 0.1024\n",
      "Epoch 40/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0267 - accuracy: 0.5852 - val_loss: 0.0736 - val_accuracy: 0.0997\n",
      "Epoch 41/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0268 - accuracy: 0.5782 - val_loss: 0.0729 - val_accuracy: 0.0945\n",
      "Epoch 42/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0258 - accuracy: 0.6046 - val_loss: 0.0737 - val_accuracy: 0.0932\n",
      "Epoch 43/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0254 - accuracy: 0.5980 - val_loss: 0.0751 - val_accuracy: 0.0906\n",
      "Epoch 44/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0253 - accuracy: 0.6156 - val_loss: 0.0744 - val_accuracy: 0.0971\n",
      "Epoch 45/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0250 - accuracy: 0.6162 - val_loss: 0.0749 - val_accuracy: 0.1063\n",
      "Epoch 46/50\n",
      "400/400 [==============================] - 13s 33ms/step - loss: 0.0246 - accuracy: 0.6218 - val_loss: 0.0756 - val_accuracy: 0.0892\n",
      "Epoch 47/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0240 - accuracy: 0.6365 - val_loss: 0.0782 - val_accuracy: 0.1076\n",
      "Epoch 48/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0242 - accuracy: 0.6339 - val_loss: 0.0767 - val_accuracy: 0.1024\n",
      "Epoch 49/50\n",
      "400/400 [==============================] - 13s 34ms/step - loss: 0.0235 - accuracy: 0.6498 - val_loss: 0.0779 - val_accuracy: 0.1181\n",
      "Epoch 50/50\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.0234 - accuracy: 0.6462 - val_loss: 0.0822 - val_accuracy: 0.1037\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(2, strides=2))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(2, strides=2))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=93, activation='softmax'))\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "history = cnn.fit(\n",
    "    train_set,\n",
    "    epochs=50,  # Number of epochs\n",
    "    validation_data=val_set,  # Pass the validation set here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(train_set.class_indices)\n",
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# # Add custom top layers\n",
    "# x = base_model.output\n",
    "# x = Flatten()(x)  # Flatten the output of the base model\n",
    "# x = Dense(4096, activation='relu')(x)  # Fully connected layer\n",
    "# x = Dense(4096, activation='relu')(x)  # Fully connected layer\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)  # Output layer for classification\n",
    "\n",
    "# # Define the complete model\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_set,\n",
    "#     epochs=10,  # Number of epochs\n",
    "#     steps_per_epoch=train_set.samples // batch_size,\n",
    "#     validation_data=val_set,  # Pass the validation set here\n",
    "#     validation_steps=val_set.samples // batch_size  # Calculate steps per epoch for validation\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
